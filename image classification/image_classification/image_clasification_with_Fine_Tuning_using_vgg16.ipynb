{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5afZC-YgtP8z"
      },
      "source": [
        "\n",
        "# Fine-Tuning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksIof2DptP81"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "\n",
        "\n",
        "In this shows how to do both Transfer Learning and Fine-Tuning using the Keras API for Tensorflow.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYd60FHAtP82"
      },
      "source": [
        "## Flowchart\n",
        "\n",
        "The idea is to re-use a pre-trained model, in this case the VGG16 model, which consists of several convolutional layers (actually blocks of multiple convolutional layers), followed by some fully-connected / dense layers and then a softmax output layer for the classification.\n",
        "\n",
        "The dense layers are responsible for combining features from the convolutional layers and this helps in the final classification. So when the VGG16 model is used on another dataset we may have to replace all the dense layers. In this case we add another dense-layer and a dropout-layer to avoid overfitting.\n",
        "\n",
        "The difference between Transfer Learning and Fine-Tuning is that in Transfer Learning we only optimize the weights of the new classification layers we have added, while we keep the weights of the original VGG16 model. In Fine-Tuning we optimize both the weights of the new classification layers we have added, as well as some or all of the layers from the VGG16 model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wc0n7tOEtP82"
      },
      "source": [
        "![Flowchart of Transfer Learning & Fine-Tuning](https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/images/10_transfer_learning_flowchart.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLhnRj-dtP82"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Athbtp0EtP82"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X59uEcIttP83"
      },
      "source": [
        "These are the imports from the Keras API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ta8UpSHjtP83"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0JalCMftP84"
      },
      "source": [
        "This was developed using Python 3.6 and TensorFlow version:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Q3jMr6zHtP84",
        "outputId": "86cf57ec-e6c2-47cf-e670-1645f65c6d5d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.1.0'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGAEhDpCtP85"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaC_vbIHtP85"
      },
      "source": [
        "### Helper-function for joining a directory and list of filenames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "US_GZ4Q5tP85"
      },
      "outputs": [],
      "source": [
        "def path_join(dirname, filenames):\n",
        "    return [os.path.join(dirname, filename) for filename in filenames]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfYtfV-atP85"
      },
      "source": [
        "### Helper-function for plotting images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpWzZW-3tP86"
      },
      "source": [
        "Function used to plot at most 9 images in a 3x3 grid, and writing the true and predicted classes below each image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMD2GnIStP86"
      },
      "outputs": [],
      "source": [
        "def plot_images(images, cls_true, cls_pred=None, smooth=True):\n",
        "\n",
        "    assert len(images) == len(cls_true)\n",
        "\n",
        "    # Create figure with sub-plots.\n",
        "    fig, axes = plt.subplots(3, 3)\n",
        "\n",
        "    # Adjust vertical spacing.\n",
        "    if cls_pred is None:\n",
        "        hspace = 0.3\n",
        "    else:\n",
        "        hspace = 0.6\n",
        "    fig.subplots_adjust(hspace=hspace, wspace=0.3)\n",
        "\n",
        "    # Interpolation type.\n",
        "    if smooth:\n",
        "        interpolation = 'spline16'\n",
        "    else:\n",
        "        interpolation = 'nearest'\n",
        "\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        # There may be less than 9 images, ensure it doesn't crash.\n",
        "        if i < len(images):\n",
        "            # Plot image.\n",
        "            ax.imshow(images[i],\n",
        "                      interpolation=interpolation)\n",
        "\n",
        "            # Name of the true class.\n",
        "            cls_true_name = class_names[cls_true[i]]\n",
        "\n",
        "            # Show true and predicted classes.\n",
        "            if cls_pred is None:\n",
        "                xlabel = \"True: {0}\".format(cls_true_name)\n",
        "            else:\n",
        "                # Name of the predicted class.\n",
        "                cls_pred_name = class_names[cls_pred[i]]\n",
        "\n",
        "                xlabel = \"True: {0}\\nPred: {1}\".format(cls_true_name, cls_pred_name)\n",
        "\n",
        "            # Show the classes as the label on the x-axis.\n",
        "            ax.set_xlabel(xlabel)\n",
        "\n",
        "        # Remove ticks from the plot.\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "\n",
        "    # Ensure the plot is shown correctly with multiple plots\n",
        "    # in a single Notebook cell.\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nminx8zutP86"
      },
      "source": [
        "### Helper-function for printing confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwUC0cx5tP86"
      },
      "outputs": [],
      "source": [
        "# Import a function from sklearn to calculate the confusion-matrix.\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def print_confusion_matrix(cls_pred):\n",
        "    # cls_pred is an array of the predicted class-number for\n",
        "    # all images in the test-set.\n",
        "\n",
        "    # Get the confusion matrix using sklearn.\n",
        "    cm = confusion_matrix(y_true=cls_test,  # True class for test-set.\n",
        "                          y_pred=cls_pred)  # Predicted class.\n",
        "\n",
        "    print(\"Confusion matrix:\")\n",
        "\n",
        "    # Print the confusion matrix as text.\n",
        "    print(cm)\n",
        "\n",
        "    # Print the class-names for easy reference.\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        print(\"({0}) {1}\".format(i, class_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8GP_4SWtP86"
      },
      "source": [
        "### Helper-function for plotting example errors\n",
        "\n",
        "Function for plotting examples of images from the test-set that have been mis-classified."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlqSIEcNtP86"
      },
      "outputs": [],
      "source": [
        "def plot_example_errors(cls_pred):\n",
        "    # cls_pred is an array of the predicted class-number for\n",
        "    # all images in the test-set.\n",
        "\n",
        "    # Boolean array whether the predicted class is incorrect.\n",
        "    incorrect = (cls_pred != cls_test)\n",
        "\n",
        "    # Get the file-paths for images that were incorrectly classified.\n",
        "    image_paths = np.array(image_paths_test)[incorrect]\n",
        "\n",
        "    # Load the first 9 images.\n",
        "    images = load_images(image_paths=image_paths[0:9])\n",
        "\n",
        "    # Get the predicted classes for those images.\n",
        "    cls_pred = cls_pred[incorrect]\n",
        "\n",
        "    # Get the true classes for those images.\n",
        "    cls_true = cls_test[incorrect]\n",
        "\n",
        "    # Plot the 9 images we have loaded and their corresponding classes.\n",
        "    # We have only loaded 9 images so there is no need to slice those again.\n",
        "    plot_images(images=images,\n",
        "                cls_true=cls_true[0:9],\n",
        "                cls_pred=cls_pred[0:9])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vS3PGIfMtP86"
      },
      "source": [
        "Function for calculating the predicted classes of the entire test-set and calling the above function to plot a few examples of mis-classified images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xlh26PVjtP87"
      },
      "outputs": [],
      "source": [
        "def example_errors():\n",
        "    # The Keras data-generator for the test-set must be reset\n",
        "    # before processing. This is because the generator will loop\n",
        "    # infinitely and keep an internal index into the dataset.\n",
        "    # So it might start in the middle of the test-set if we do\n",
        "    # not reset it first. This makes it impossible to match the\n",
        "    # predicted classes with the input images.\n",
        "    # If we reset the generator, then it always starts at the\n",
        "    # beginning so we know exactly which input-images were used.\n",
        "    generator_test.reset()\n",
        "\n",
        "    # Predict the classes for all images in the test-set.\n",
        "    y_pred = new_model.predict(generator_test, steps=steps_test)\n",
        "\n",
        "    # Convert the predicted classes from arrays to integers.\n",
        "    cls_pred = np.argmax(y_pred,axis=1)\n",
        "\n",
        "    # Plot examples of mis-classified images.\n",
        "    plot_example_errors(cls_pred)\n",
        "\n",
        "    # Print the confusion matrix.\n",
        "    print_confusion_matrix(cls_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybbZ_ZFstP87"
      },
      "source": [
        "### Helper-function for loading images\n",
        "\n",
        "The data-set is not loaded into memory, instead it has a list of the files for the images in the training-set and another list of the files for the images in the test-set. This helper-function loads some image-files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHSNQj-xtP87"
      },
      "outputs": [],
      "source": [
        "def load_images(image_paths):\n",
        "    # Load the images from disk.\n",
        "    images = [plt.imread(path) for path in image_paths]\n",
        "\n",
        "    # Convert to a numpy array and return it.\n",
        "    return np.asarray(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4vkKbFgtP87"
      },
      "source": [
        "### Helper-function for plotting training history\n",
        "\n",
        "This plots the classification accuracy and loss-values recorded during training with the Keras API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApU2NpA7tP87"
      },
      "outputs": [],
      "source": [
        "def plot_training_history(history):\n",
        "    # Get the classification accuracy and loss-value\n",
        "    # for the training-set.\n",
        "    acc = history.history['categorical_accuracy']\n",
        "    loss = history.history['loss']\n",
        "\n",
        "    # Get it for the validation-set (we only use the test-set).\n",
        "    val_acc = history.history['val_categorical_accuracy']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    # Plot the accuracy and loss-values for the training-set.\n",
        "    plt.plot(acc, linestyle='-', color='b', label='Training Acc.')\n",
        "    plt.plot(loss, 'o', color='b', label='Training Loss')\n",
        "\n",
        "    # Plot it for the test-set.\n",
        "    plt.plot(val_acc, linestyle='--', color='r', label='Test Acc.')\n",
        "    plt.plot(val_loss, 'o', color='r', label='Test Loss')\n",
        "\n",
        "    # Plot title and legend.\n",
        "    plt.title('Training and Test Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Ensure the plot shows correctly.\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "\n",
        "dataset = \"/content/drive/MyDrive/ChartQA Dataset/CQA/train/png\"\n",
        "data_dir = pathlib.Path(data_dir).with_suffix('')"
      ],
      "metadata": {
        "id": "575hNovWuYIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
        "print(image_count)"
      ],
      "metadata": {
        "id": "HWfx_6zGufE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roses = list(data_dir.glob('line chart/*'))\n",
        "PIL.Image.open(str(roses[0]))"
      ],
      "metadata": {
        "id": "4nQs3wE_ufHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PIL.Image.open(str(line chart[1]))"
      ],
      "metadata": {
        "id": "CBuiD72uufKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tulips = list(data_dir.glob('bar chart/*'))\n",
        "PIL.Image.open(str(tulips[0]))"
      ],
      "metadata": {
        "id": "rQ2bDQbVusm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PIL.Image.open(str(bar chart[1]))"
      ],
      "metadata": {
        "id": "OwFKOuSvusp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gACrDaEFustF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FVVl1Ieiusv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SmmlR4fdus5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0E_xx32ftP88"
      },
      "source": [
        "## Pre-Trained Model: VGG16\n",
        "\n",
        "The following creates an instance of the pre-trained VGG16 model using the Keras API. This automatically downloads the required files if you don't have them already. Note how simple this is in Keras compared to Tutorial #08.\n",
        "\n",
        "The VGG16 model contains a convolutional part and a fully-connected (or dense) part which is used for classification. If `include_top=True` then the whole VGG16 model is downloaded which is about 528 MB. If `include_top=False` then only the convolutional part of the VGG16 model is downloaded which is just 57 MB.\n",
        "\n",
        "We will try and use the pre-trained model for predicting the class of some images in our new dataset, so we have to download the full model, but if you have a slow internet connection, then you can modify the code below to use the smaller pre-trained model without the classification layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "2n5ICFr3tP88"
      },
      "outputs": [],
      "source": [
        "model = VGG16(include_top=True, weights='imagenet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGXin_x6tP88"
      },
      "source": [
        "## Input Pipeline\n",
        "\n",
        "The Keras API has its own way of creating the input pipeline for training a model using files.\n",
        "\n",
        "First we need to know the shape of the tensors expected as input by the pre-trained VGG16 model. In this case it is images of shape 224 x 224 x 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dfh9eSGFtP88",
        "outputId": "bf045c8a-8fc1-48af-97af-5d4ff94533b6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(224, 224)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_shape = model.layers[0].output_shape[0][1:3]\n",
        "input_shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99kEHSMBtP89"
      },
      "source": [
        "Keras uses a so-called data-generator for inputting data into the neural network, which will loop over the data for eternity.\n",
        "\n",
        "We have a small training-set so it helps to artificially inflate its size by making various transformations to the images. We use a built-in data-generator that can make these random transformations. This is also called an augmented dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GL8zXzSItP9B"
      },
      "outputs": [],
      "source": [
        "datagen_train = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=180,\n",
        "      width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      shear_range=0.1,\n",
        "      zoom_range=[0.9, 1.5],\n",
        "      horizontal_flip=True,\n",
        "      vertical_flip=True,\n",
        "      fill_mode='nearest')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFFB3c5gtP9B"
      },
      "source": [
        "We also need a data-generator for the test-set, but this should not do any transformations to the images because we want to know the exact classification accuracy on those specific images. So we just rescale the pixel-values so they are between 0.0 and 1.0 because this is expected by the VGG16 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCUL_glQtP9B"
      },
      "outputs": [],
      "source": [
        "datagen_test = ImageDataGenerator(rescale=1./255)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqQBTc5mtP9B"
      },
      "source": [
        "The data-generators will return batches of images. Because the VGG16 model is so large, the batch-size cannot be too large, otherwise you will run out of RAM on the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oArdVvy5tP9B"
      },
      "outputs": [],
      "source": [
        "batch_size = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfPr9-_ttP9B"
      },
      "source": [
        "We can save the randomly transformed images during training, so as to inspect whether they have been overly distorted, so we have to adjust the parameters for the data-generator above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAOYXhmStP9B"
      },
      "outputs": [],
      "source": [
        "if True:\n",
        "    save_to_dir = None\n",
        "else:\n",
        "    save_to_dir='augmented_images/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvcQpJk4tP9C"
      },
      "source": [
        "Now we create the actual data-generator that will read files from disk, resize the images and return a random batch.\n",
        "\n",
        "It is somewhat awkward that the construction of the data-generator is split into these two steps, but it is probably because there are different kinds of data-generators available for different data-types (images, text, etc.) and sources (memory or disk)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "VeHDLBiCtP9C",
        "outputId": "42000df1-f441-49ae-d384-df0ddf68d337"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 4170 images belonging to 3 classes.\n"
          ]
        }
      ],
      "source": [
        "generator_train = datagen_train.flow_from_directory(directory=train_dir,\n",
        "                                                    target_size=input_shape,\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    shuffle=True,\n",
        "                                                    save_to_dir=save_to_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYEIwtw4tP9C"
      },
      "source": [
        "The data-generator for the test-set should not transform and shuffle the images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioszhyUWtP9C",
        "outputId": "ec073b9d-7b9c-4c03-d3d1-6dc1ae963e1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 530 images belonging to 3 classes.\n"
          ]
        }
      ],
      "source": [
        "generator_test = datagen_test.flow_from_directory(directory=test_dir,\n",
        "                                                  target_size=input_shape,\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjddgCRStP9C"
      },
      "source": [
        "Because the data-generators will loop for eternity, we need to specify the number of steps to perform during evaluation and prediction on the test-set. Because our test-set contains 530 images and the batch-size is set to 20, the number of steps is 26.5 for one full processing of the test-set. This is why we need to reset the data-generator's counter in the `example_errors()` function above, so it always starts processing from the beginning of the test-set.\n",
        "\n",
        "This is another slightly awkward aspect of the Keras API which could perhaps be improved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLTwt1YktP9C",
        "outputId": "d4345a8d-ee12-48dd-8820-2346ab4d1e8b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "26.5"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "steps_test = generator_test.n / batch_size\n",
        "steps_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t9AxD4ttP9C"
      },
      "source": [
        "Get the file-paths for all the images in the training- and test-sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zS0PEW0otP9D"
      },
      "outputs": [],
      "source": [
        "image_paths_train = path_join(train_dir, generator_train.filenames)\n",
        "image_paths_test = path_join(test_dir, generator_test.filenames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a9cWuWhtP9D"
      },
      "source": [
        "Get the class-numbers for all the images in the training- and test-sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBxgkNd2tP9D"
      },
      "outputs": [],
      "source": [
        "cls_train = generator_train.classes\n",
        "cls_test = generator_test.classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pj76uvTMtP9D"
      },
      "source": [
        "Get the class-names for the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxwforeHtP9D"
      },
      "source": [
        "Get the number of classes for the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3RoouI6tP9E",
        "outputId": "548b560d-4595-4184-f61b-7ef3134c2e82"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_classes = generator_train.num_classes\n",
        "num_classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_TNmsjgtP9F"
      },
      "source": [
        "### Class Weights\n",
        "\n",
        "The Knifey-Spoony dataset is quite imbalanced because it has few images of forks, more images of knives, and many more images of spoons. This can cause a problem during training because the neural network will be shown many more examples of spoons than forks, so it might become better at recognizing spoons.\n",
        "\n",
        "Here we use scikit-learn to calculate weights that will properly balance the dataset. These weights are applied to the gradient for each image in the batch during training, so as to scale their influence on the overall gradient for the batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfbN_2jNtP9F"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ctKZf1VtP9F"
      },
      "outputs": [],
      "source": [
        "class_weight = compute_class_weight(class_weight='balanced',\n",
        "                                    classes=np.unique(cls_train),\n",
        "                                    y=cls_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMYWzuGFtP9F",
        "outputId": "4c94faca-189d-4e35-adb2-6b8e4815b5bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1.39839034, 1.14876033, 0.70701933])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class_weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TVp3GY_tP9G"
      },
      "source": [
        "## Example Predictions\n",
        "\n",
        "Here we will show a few examples of using the pre-trained VGG16 model for prediction.\n",
        "\n",
        "We need a helper-function for loading and resizing an image so it can be input to the VGG16 model, as well as doing the actual prediction and showing the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hyGSlbStP9G"
      },
      "outputs": [],
      "source": [
        "def predict(image_path):\n",
        "    # Load and resize the image using PIL.\n",
        "    img = PIL.Image.open(image_path)\n",
        "    img_resized = img.resize(input_shape, PIL.Image.LANCZOS)\n",
        "\n",
        "    # Plot the image.\n",
        "    plt.imshow(img_resized)\n",
        "    plt.show()\n",
        "\n",
        "    # Convert the PIL image to a numpy-array with the proper shape.\n",
        "    img_array = np.expand_dims(np.array(img_resized), axis=0)\n",
        "\n",
        "    # Use the VGG16 model to make a prediction.\n",
        "    # This outputs an array with 1000 numbers corresponding to\n",
        "    # the classes of the ImageNet-dataset.\n",
        "    pred = model.predict(img_array)\n",
        "\n",
        "    # Decode the output of the VGG16 model.\n",
        "    pred_decoded = decode_predictions(pred)[0]\n",
        "\n",
        "    # Print the predictions.\n",
        "    for code, name, score in pred_decoded:\n",
        "        print(\"{0:>6.2%} : {1}\".format(score, name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiuyF4hrtP9G"
      },
      "source": [
        "We can then use the VGG16 model on a picture of a parrot which is classified as a macaw (a parrot species) with a fairly high score of 79%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7Tjkt_rtP9G"
      },
      "source": [
        "We can then use the VGG16 model to predict the class of one of the images in our new training-set. The VGG16 model is very confused about this image and cannot make a good classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXtImOtxtP9I"
      },
      "source": [
        "We can try it for another image in our new training-set and the VGG16 model is still confused."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tImiMgvZtP9I"
      },
      "source": [
        "We can also try an image from our new test-set, and again the VGG16 model is very confused."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DYJo1o5tP9I"
      },
      "source": [
        "## Transfer Learning\n",
        "\n",
        "The pre-trained VGG16 model was unable to classify images from the Knifey-Spoony dataset. The reason is perhaps that the VGG16 model was trained on the so-called ImageNet dataset which may not have contained many images of cutlery.\n",
        "\n",
        "The lower layers of a Convolutional Neural Network can recognize many different shapes or features in an image. It is the last few fully-connected layers that combine these featuers into classification of a whole image. So we can try and re-route the output of the last convolutional layer of the VGG16 model to a new fully-connected neural network that we create for doing classification on the Knifey-Spoony dataset.\n",
        "\n",
        "First we print a summary of the VGG16 model so we can see the names and types of its layers, as well as the shapes of the tensors flowing between the layers. This is one of the major reasons we are using the VGG16 model in this tutorial, because the Inception v3 model has so many layers that it is confusing when printed out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "aaSMiapttP9J",
        "outputId": "6993ec6d-252b-43f3-e441-52bc1f80fb7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "predictions (Dense)          (None, 1000)              4097000   \n",
            "=================================================================\n",
            "Total params: 138,357,544\n",
            "Trainable params: 138,357,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6Hxq_tRtP9J"
      },
      "source": [
        "We can see that the last convolutional layer is called 'block5_pool' so we use Keras to get a reference to that layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-v0Dki8itP9J"
      },
      "outputs": [],
      "source": [
        "transfer_layer = model.get_layer('block5_pool')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0-D2bHTtP9J"
      },
      "source": [
        "We refer to this layer as the Transfer Layer because its output will be re-routed to our new fully-connected neural network which will do the classification for the Knifey-Spoony dataset.\n",
        "\n",
        "The output of the transfer layer has the following shape:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jwg7MYetP9J",
        "outputId": "6c055db4-66f2-4a16-b012-08fe5f688005"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor 'block5_pool/Identity:0' shape=(None, 7, 7, 512) dtype=float32>"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transfer_layer.output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L66Esz5etP9J"
      },
      "source": [
        "Using the Keras API it is very simple to create a new model. First we take the part of the VGG16 model from its input-layer to the output of the transfer-layer. We may call this the convolutional model, because it consists of all the convolutional layers from the VGG16 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZpWoGurtP9J"
      },
      "outputs": [],
      "source": [
        "conv_model = Model(inputs=model.input,\n",
        "                   outputs=transfer_layer.output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOhepAnktP9J"
      },
      "source": [
        "We can then use Keras to build a new model on top of this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "K8pwT5UFtP9J"
      },
      "outputs": [],
      "source": [
        "# Start a new Keras Sequential model.\n",
        "new_model = Sequential()\n",
        "\n",
        "# Add the convolutional part of the VGG16 model from above.\n",
        "new_model.add(conv_model)\n",
        "\n",
        "# Flatten the output of the VGG16 model because it is from a\n",
        "# convolutional layer.\n",
        "new_model.add(Flatten())\n",
        "\n",
        "# Add a dense (aka. fully-connected) layer.\n",
        "# This is for combining features that the VGG16 model has\n",
        "# recognized in the image.\n",
        "new_model.add(Dense(1024, activation='relu'))\n",
        "\n",
        "# Add a dropout-layer which may prevent overfitting and\n",
        "# improve generalization ability to unseen data e.g. the test-set.\n",
        "new_model.add(Dropout(0.5))\n",
        "\n",
        "# Add the final layer for the actual classification.\n",
        "new_model.add(Dense(num_classes, activation='softmax'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xK6tJlPstP9K"
      },
      "source": [
        "We use the Adam optimizer with a fairly low learning-rate. The learning-rate could perhaps be larger. But if you try and train more layers of the original VGG16 model, then the learning-rate should be quite low otherwise the pre-trained weights of the VGG16 model will be distorted and it will be unable to learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hCMejaytP9K"
      },
      "outputs": [],
      "source": [
        "optimizer = Adam(lr=1e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdtB7sThtP9K"
      },
      "source": [
        "We have 4 classes in the  dataset so Keras needs to use this loss-function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVrcycxNtP9K"
      },
      "outputs": [],
      "source": [
        "loss = 'categorical_crossentropy'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5_ly9TmtP9K"
      },
      "source": [
        "The only performance metric we are interested in is the classification accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AwSle2ntP9K"
      },
      "outputs": [],
      "source": [
        "metrics = ['categorical_accuracy']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-uYqj6mtP9K"
      },
      "source": [
        "Helper-function for printing whether a layer in the VGG16 model should be trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6qVbVWetP9L"
      },
      "outputs": [],
      "source": [
        "def print_layer_trainable():\n",
        "    for layer in conv_model.layers:\n",
        "        print(\"{0}:\\t{1}\".format(layer.trainable, layer.name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vfrn76GdtP9L"
      },
      "source": [
        "By default all the layers of the VGG16 model are trainable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "6jYzjwbttP9L",
        "outputId": "79154412-5cac-47e4-c607-9f08405cf9e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True:\tinput_1\n",
            "True:\tblock1_conv1\n",
            "True:\tblock1_conv2\n",
            "True:\tblock1_pool\n",
            "True:\tblock2_conv1\n",
            "True:\tblock2_conv2\n",
            "True:\tblock2_pool\n",
            "True:\tblock3_conv1\n",
            "True:\tblock3_conv2\n",
            "True:\tblock3_conv3\n",
            "True:\tblock3_pool\n",
            "True:\tblock4_conv1\n",
            "True:\tblock4_conv2\n",
            "True:\tblock4_conv3\n",
            "True:\tblock4_pool\n",
            "True:\tblock5_conv1\n",
            "True:\tblock5_conv2\n",
            "True:\tblock5_conv3\n",
            "True:\tblock5_pool\n"
          ]
        }
      ],
      "source": [
        "print_layer_trainable()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pMe75j0tP9L"
      },
      "source": [
        "In Transfer Learning we are initially only interested in reusing the pre-trained VGG16 model as it is, so we will disable training for all its layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2faZrdxtP9L"
      },
      "outputs": [],
      "source": [
        "conv_model.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DT6bi1_tP9L"
      },
      "outputs": [],
      "source": [
        "for layer in conv_model.layers:\n",
        "    layer.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6hD38cQtP9M",
        "outputId": "26b389b6-7561-4ac8-a3ca-19020d13aac7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False:\tinput_1\n",
            "False:\tblock1_conv1\n",
            "False:\tblock1_conv2\n",
            "False:\tblock1_pool\n",
            "False:\tblock2_conv1\n",
            "False:\tblock2_conv2\n",
            "False:\tblock2_pool\n",
            "False:\tblock3_conv1\n",
            "False:\tblock3_conv2\n",
            "False:\tblock3_conv3\n",
            "False:\tblock3_pool\n",
            "False:\tblock4_conv1\n",
            "False:\tblock4_conv2\n",
            "False:\tblock4_conv3\n",
            "False:\tblock4_pool\n",
            "False:\tblock5_conv1\n",
            "False:\tblock5_conv2\n",
            "False:\tblock5_conv3\n",
            "False:\tblock5_pool\n"
          ]
        }
      ],
      "source": [
        "print_layer_trainable()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq_zXksdtP9M"
      },
      "source": [
        "Once we have changed whether the model's layers are trainable, we need to compile the model for the changes to take effect."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFc7RUIJtP9M"
      },
      "outputs": [],
      "source": [
        "new_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og7EJjnXtP9M"
      },
      "source": [
        "An epoch normally means one full processing of the training-set. But the data-generator that we created above, will produce batches of training-data for eternity. So we need to define the number of steps we want to run for each \"epoch\" and this number gets multiplied by the batch-size defined above. In this case we have 100 steps per epoch and a batch-size of 20, so the \"epoch\" consists of 2000 random images from the training-set. We run 20 such \"epochs\".\n",
        "\n",
        "The reason these particular numbers were chosen, was because they seemed to be sufficient for training with this particular model and dataset, and it didn't take too much time, and resulted in 20 data-points (one for each \"epoch\") which can be plotted afterwards."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67TGc7IptP9M"
      },
      "outputs": [],
      "source": [
        "epochs = 20\n",
        "steps_per_epoch = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0QJv6R8tP9M"
      },
      "source": [
        "Training the new model is just a single function call in the Keras API. This takes about 6-7 minutes on a GTX 1070 GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "t0LWa6ektP9M",
        "outputId": "5308e18d-0a1e-4e1f-9fa1-cea59c206105"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "Train for 100 steps, validate for 26.5 steps\n",
            "Epoch 1/20\n",
            "100/100 [==============================] - 32s 322ms/step - loss: 1.1149 - categorical_accuracy: 0.4585 - val_loss: 0.8945 - val_categorical_accuracy: 0.5604\n",
            "Epoch 2/20\n",
            "100/100 [==============================] - 31s 309ms/step - loss: 0.9385 - categorical_accuracy: 0.5432 - val_loss: 0.7380 - val_categorical_accuracy: 0.7717\n",
            "Epoch 3/20\n",
            "100/100 [==============================] - 29s 285ms/step - loss: 0.8408 - categorical_accuracy: 0.6186 - val_loss: 0.6924 - val_categorical_accuracy: 0.7321\n",
            "Epoch 4/20\n",
            "100/100 [==============================] - 30s 299ms/step - loss: 0.7855 - categorical_accuracy: 0.6432 - val_loss: 0.6994 - val_categorical_accuracy: 0.7057\n",
            "Epoch 5/20\n",
            "100/100 [==============================] - 29s 294ms/step - loss: 0.7128 - categorical_accuracy: 0.6879 - val_loss: 0.7109 - val_categorical_accuracy: 0.6698\n",
            "Epoch 6/20\n",
            "100/100 [==============================] - 29s 286ms/step - loss: 0.6945 - categorical_accuracy: 0.7015 - val_loss: 0.6150 - val_categorical_accuracy: 0.7604\n",
            "Epoch 7/20\n",
            "100/100 [==============================] - 28s 282ms/step - loss: 0.6910 - categorical_accuracy: 0.7060 - val_loss: 0.6316 - val_categorical_accuracy: 0.7321\n",
            "Epoch 8/20\n",
            "100/100 [==============================] - 28s 284ms/step - loss: 0.6269 - categorical_accuracy: 0.7382 - val_loss: 0.5828 - val_categorical_accuracy: 0.7868\n",
            "Epoch 9/20\n",
            "100/100 [==============================] - 30s 300ms/step - loss: 0.6180 - categorical_accuracy: 0.7362 - val_loss: 0.6337 - val_categorical_accuracy: 0.7377\n",
            "Epoch 10/20\n",
            "100/100 [==============================] - 30s 297ms/step - loss: 0.5823 - categorical_accuracy: 0.7568 - val_loss: 0.5569 - val_categorical_accuracy: 0.7868\n",
            "Epoch 11/20\n",
            "100/100 [==============================] - 30s 295ms/step - loss: 0.5969 - categorical_accuracy: 0.7455 - val_loss: 0.6298 - val_categorical_accuracy: 0.7340\n",
            "Epoch 12/20\n",
            "100/100 [==============================] - 29s 289ms/step - loss: 0.5516 - categorical_accuracy: 0.7704 - val_loss: 0.5804 - val_categorical_accuracy: 0.7566\n",
            "Epoch 13/20\n",
            "100/100 [==============================] - 30s 297ms/step - loss: 0.5514 - categorical_accuracy: 0.7770 - val_loss: 0.5879 - val_categorical_accuracy: 0.7453\n",
            "Epoch 14/20\n",
            "100/100 [==============================] - 33s 326ms/step - loss: 0.5239 - categorical_accuracy: 0.7830 - val_loss: 0.5448 - val_categorical_accuracy: 0.7849\n",
            "Epoch 15/20\n",
            "100/100 [==============================] - 32s 325ms/step - loss: 0.5367 - categorical_accuracy: 0.7760 - val_loss: 0.6596 - val_categorical_accuracy: 0.7226\n",
            "Epoch 16/20\n",
            "100/100 [==============================] - 28s 282ms/step - loss: 0.5155 - categorical_accuracy: 0.7860 - val_loss: 0.5385 - val_categorical_accuracy: 0.7755\n",
            "Epoch 17/20\n",
            "100/100 [==============================] - 29s 289ms/step - loss: 0.5058 - categorical_accuracy: 0.7889 - val_loss: 0.6200 - val_categorical_accuracy: 0.7340\n",
            "Epoch 18/20\n",
            "100/100 [==============================] - 28s 283ms/step - loss: 0.4925 - categorical_accuracy: 0.8030 - val_loss: 0.6469 - val_categorical_accuracy: 0.7151\n",
            "Epoch 19/20\n",
            "100/100 [==============================] - 31s 312ms/step - loss: 0.4681 - categorical_accuracy: 0.8145 - val_loss: 0.7350 - val_categorical_accuracy: 0.6906\n",
            "Epoch 20/20\n",
            "100/100 [==============================] - 31s 307ms/step - loss: 0.4743 - categorical_accuracy: 0.8045 - val_loss: 0.5995 - val_categorical_accuracy: 0.7377\n"
          ]
        }
      ],
      "source": [
        "history = new_model.fit(x=generator_train,\n",
        "                        epochs=epochs,\n",
        "                        steps_per_epoch=steps_per_epoch,\n",
        "                        class_weight=class_weight,\n",
        "                        validation_data=generator_test,\n",
        "                        validation_steps=steps_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NL08RLLutP9M"
      },
      "source": [
        "Keras records the performance metrics at the end of each \"epoch\" so they can be plotted later. This shows that the loss-value for the training-set generally decreased during training, but the loss-values for the test-set were a bit more erratic. Similarly, the classification accuracy generally improved on the training-set while it was a bit more erratic on the test-set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Z3S5cDpftP9N",
        "outputId": "127aeb87-5a67-4b23-8afb-d0e4e856fe65"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_training_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjhI8062tP9N"
      },
      "source": [
        "After training we can also evaluate the new model's performance on the test-set using a single function call in the Keras API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "7K68PS6VtP9N",
        "outputId": "33c924d7-2b9f-4fe1-91f0-82545ec63ff6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "27/26 [==============================] - 4s 156ms/step - loss: 0.5888 - categorical_accuracy: 0.7377\n"
          ]
        }
      ],
      "source": [
        "result = new_model.evaluate(generator_test, steps=steps_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "34RVM3-utP9N",
        "outputId": "b9e4eb08-d8db-41ad-863a-74cd7059e478"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test-set classification accuracy: 73.77%\n"
          ]
        }
      ],
      "source": [
        "print(\"Test-set classification accuracy: {0:.2%}\".format(result[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGZrzh4KtP9N"
      },
      "source": [
        "We can plot some examples of mis-classified images from the test-set. Some of these images are also difficult for a human to classify.\n",
        "\n",
        "The confusion matrix shows that the new model is especially having problems classifying the forky-class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7sH0UGDtP9N"
      },
      "source": [
        "## Fine-Tuning\n",
        "\n",
        "In Transfer Learning the original pre-trained model is locked or frozen during training of the new classifier. This ensures that the weights of the original VGG16 model will not change. One advantage of this, is that the training of the new classifier will not propagate large gradients back through the VGG16 model that may either distort its weights or cause overfitting to the new dataset.\n",
        "\n",
        "But once the new classifier has been trained we can try and gently fine-tune some of the deeper layers in the VGG16 model as well. We call this Fine-Tuning.\n",
        "\n",
        "It is a bit unclear whether Keras uses the `trainable` boolean in each layer of the original VGG16 model or if it is overrided by the `trainable` boolean in the \"meta-layer\" we call `conv_layer`. So we will enable the `trainable` boolean for both `conv_layer` and all the relevant layers in the original VGG16 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZFs2HTntP9N"
      },
      "outputs": [],
      "source": [
        "conv_model.trainable = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7D0m4kMKtP9O"
      },
      "source": [
        "We want to train the last two convolutional layers whose names contain 'block5' or 'block4'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nRNV8WJtP9O"
      },
      "outputs": [],
      "source": [
        "for layer in conv_model.layers:\n",
        "    # Boolean whether this layer is trainable.\n",
        "    trainable = ('block5' in layer.name or 'block4' in layer.name)\n",
        "\n",
        "    # Set the layer's bool.\n",
        "    layer.trainable = trainable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtMeu8vMtP9O"
      },
      "source": [
        "We can check that this has updated the `trainable` boolean for the relevant layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5mNuFtntP9O",
        "outputId": "da941e08-5a0c-4fb1-a8c3-e594128fe739"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False:\tinput_1\n",
            "False:\tblock1_conv1\n",
            "False:\tblock1_conv2\n",
            "False:\tblock1_pool\n",
            "False:\tblock2_conv1\n",
            "False:\tblock2_conv2\n",
            "False:\tblock2_pool\n",
            "False:\tblock3_conv1\n",
            "False:\tblock3_conv2\n",
            "False:\tblock3_conv3\n",
            "False:\tblock3_pool\n",
            "True:\tblock4_conv1\n",
            "True:\tblock4_conv2\n",
            "True:\tblock4_conv3\n",
            "True:\tblock4_pool\n",
            "True:\tblock5_conv1\n",
            "True:\tblock5_conv2\n",
            "True:\tblock5_conv3\n",
            "True:\tblock5_pool\n"
          ]
        }
      ],
      "source": [
        "print_layer_trainable()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOcByeLftP9O"
      },
      "source": [
        "We will use a lower learning-rate for the fine-tuning so the weights of the original VGG16 model only get changed slowly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQRsuae2tP9O"
      },
      "outputs": [],
      "source": [
        "optimizer_fine = Adam(lr=1e-7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OE1QgmlptP9O"
      },
      "source": [
        "Because we have defined a new optimizer and have changed the `trainable` boolean for many of the layers in the model, we need to recompile the model so the changes can take effect before we continue training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHc7ZscktP9O"
      },
      "outputs": [],
      "source": [
        "new_model.compile(optimizer=optimizer_fine, loss=loss, metrics=metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coYhWooutP9O"
      },
      "source": [
        "The training can then be continued so as to fine-tune the VGG16 model along with the new classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "IjBuuVK0tP9O",
        "outputId": "8ce964c6-fc1e-4d3c-832f-92d09f13f29a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "Train for 100 steps, validate for 26.5 steps\n",
            "Epoch 1/20\n",
            "100/100 [==============================] - 34s 339ms/step - loss: 0.4605 - categorical_accuracy: 0.8211 - val_loss: 0.5776 - val_categorical_accuracy: 0.7566\n",
            "Epoch 2/20\n",
            "100/100 [==============================] - 38s 384ms/step - loss: 0.4683 - categorical_accuracy: 0.8175 - val_loss: 0.5600 - val_categorical_accuracy: 0.7604\n",
            "Epoch 3/20\n",
            "100/100 [==============================] - 36s 357ms/step - loss: 0.4643 - categorical_accuracy: 0.8095 - val_loss: 0.5748 - val_categorical_accuracy: 0.7528\n",
            "Epoch 4/20\n",
            "100/100 [==============================] - 37s 371ms/step - loss: 0.4368 - categorical_accuracy: 0.8236 - val_loss: 0.5613 - val_categorical_accuracy: 0.7604\n",
            "Epoch 5/20\n",
            "100/100 [==============================] - 37s 367ms/step - loss: 0.4140 - categorical_accuracy: 0.8317 - val_loss: 0.5490 - val_categorical_accuracy: 0.7642\n",
            "Epoch 6/20\n",
            "100/100 [==============================] - 44s 439ms/step - loss: 0.4456 - categorical_accuracy: 0.8155 - val_loss: 0.5488 - val_categorical_accuracy: 0.7660\n",
            "Epoch 7/20\n",
            "100/100 [==============================] - 45s 454ms/step - loss: 0.4318 - categorical_accuracy: 0.8352 - val_loss: 0.5505 - val_categorical_accuracy: 0.7660\n",
            "Epoch 8/20\n",
            "100/100 [==============================] - 41s 409ms/step - loss: 0.4283 - categorical_accuracy: 0.8265 - val_loss: 0.5580 - val_categorical_accuracy: 0.7604\n",
            "Epoch 9/20\n",
            "100/100 [==============================] - 43s 427ms/step - loss: 0.4197 - categorical_accuracy: 0.8392 - val_loss: 0.5496 - val_categorical_accuracy: 0.7679\n",
            "Epoch 10/20\n",
            "100/100 [==============================] - 43s 431ms/step - loss: 0.4138 - categorical_accuracy: 0.8312 - val_loss: 0.5535 - val_categorical_accuracy: 0.7679\n",
            "Epoch 11/20\n",
            "100/100 [==============================] - 38s 378ms/step - loss: 0.4373 - categorical_accuracy: 0.8332 - val_loss: 0.5449 - val_categorical_accuracy: 0.7679\n",
            "Epoch 12/20\n",
            "100/100 [==============================] - 25s 252ms/step - loss: 0.4046 - categorical_accuracy: 0.8470 - val_loss: 0.5396 - val_categorical_accuracy: 0.7698\n",
            "Epoch 13/20\n",
            "100/100 [==============================] - 23s 234ms/step - loss: 0.4014 - categorical_accuracy: 0.8442 - val_loss: 0.5400 - val_categorical_accuracy: 0.7717\n",
            "Epoch 14/20\n",
            "100/100 [==============================] - 24s 237ms/step - loss: 0.4141 - categorical_accuracy: 0.8365 - val_loss: 0.5473 - val_categorical_accuracy: 0.7679\n",
            "Epoch 15/20\n",
            "100/100 [==============================] - 23s 231ms/step - loss: 0.4117 - categorical_accuracy: 0.8307 - val_loss: 0.5436 - val_categorical_accuracy: 0.7698\n",
            "Epoch 16/20\n",
            "100/100 [==============================] - 23s 229ms/step - loss: 0.3826 - categorical_accuracy: 0.8472 - val_loss: 0.5549 - val_categorical_accuracy: 0.7623\n",
            "Epoch 17/20\n",
            "100/100 [==============================] - 23s 228ms/step - loss: 0.3979 - categorical_accuracy: 0.8442 - val_loss: 0.5402 - val_categorical_accuracy: 0.7698\n",
            "Epoch 18/20\n",
            "100/100 [==============================] - 24s 239ms/step - loss: 0.3941 - categorical_accuracy: 0.8447 - val_loss: 0.5313 - val_categorical_accuracy: 0.7774\n",
            "Epoch 19/20\n",
            "100/100 [==============================] - 24s 244ms/step - loss: 0.3956 - categorical_accuracy: 0.8385 - val_loss: 0.5407 - val_categorical_accuracy: 0.7698\n",
            "Epoch 20/20\n",
            "100/100 [==============================] - 24s 240ms/step - loss: 0.4037 - categorical_accuracy: 0.8281 - val_loss: 0.5352 - val_categorical_accuracy: 0.7755\n"
          ]
        }
      ],
      "source": [
        "history = new_model.fit(x=generator_train,\n",
        "                        epochs=epochs,\n",
        "                        steps_per_epoch=steps_per_epoch,\n",
        "                        class_weight=class_weight,\n",
        "                        validation_data=generator_test,\n",
        "                        validation_steps=steps_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4l5Hf2LtP9O"
      },
      "source": [
        "We can then plot the loss-values and classification accuracy from the training. Depending on the dataset, the original model, the new classifier, and hyper-parameters such as the learning-rate, this may improve the classification accuracies on both training- and test-set, or it may improve on the training-set but worsen it for the test-set in case of overfitting. It may require some experimentation with the parameters to get this right."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53jD7h3atP9P",
        "outputId": "e9b3f800-64cb-41f8-98e5-bb2f1bb51a1d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_training_history(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MsJNoLbtP9P",
        "outputId": "0606cf3a-a5e4-487e-a438-0b3d4cc2a1cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "27/26 [==============================] - 3s 118ms/step - loss: 0.5256 - categorical_accuracy: 0.7755\n"
          ]
        }
      ],
      "source": [
        "result = new_model.evaluate(generator_test, steps=steps_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1hxE30otP9P",
        "outputId": "8a81a660-27fb-45e8-b080-d7ea315c6e27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test-set classification accuracy: 77.55%\n"
          ]
        }
      ],
      "source": [
        "print(\"Test-set classification accuracy: {0:.2%}\".format(result[1]))"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}